"""
    Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs
    http://arxiv.org/abs/1711.09869
    2017 Loic Landrieu, Martin Simonovsky
"""
from __future__ import division
from __future__ import print_function
from builtins import range

import torch
import torch.nn as nn
import torch.nn.init as init        #torch.nn.init 为初始化库
from learning import ecc
from learning.modules import RNNGraphConvModule, ECC_CRFModule, GRUCellEx, LSTMCellEx


def create_fnet(widths, orthoinit, llbias, bnidx=-1):
    """ Creates feature-generating network, a multi-layer perceptron.
    Parameters:
    widths: list of widths of layers (including input and output widths)
    orthoinit: whether to use orthogonal weight initialization
    llbias: whether to use bias in the last layer
    bnidx: index of batch normalization (-1 if not used)
    """
    """创建特征生成网络，即多层感知器。
         参数：
         widths：图层宽度列表（包括输入和输出宽度）
         orthoinit：是否使用正交权重初始化
         llbias：是否在最后一层使用偏差
         bnidx：批处理规范化的索引（如果未使用，则为 - 1）
    """
    fnet_modules = []

#………………………………………………………………………………………………………………重写
#Creates feature-generating network, a multi-layer perceptron.
    #i=0
    fnet_modules.append(nn.Linear(13,32))
    fnet_modules.append(nn.ReLU(True))
    #i=1
    fnet_modules.append(nn.Linear(32,128))
    fnet_modules.append(nn.ReLU(True))
    #i=2
    fnet_modules.append(nn.Linear(128,64))
    fnet_modules.append(nn.BatchNorm1d(widths[64]))
    fnet_modules.append(nn.ReLU(True))
    #i=3
    fnet_modules.append(nn.Linear(64,1024,bias=llbias))

    return nn.Sequential(*fnet_modules)
#…………………………………………………………………………………………………………………………………………………………………重写

    #for k in range(len(widths)-2):    #添加对数据进行处理的标准化层、线性变换函数(nn.Linear)等
        #fnet_modules.append(nn.Linear(widths[k], widths[k+1]))
        #if orthoinit: init.orthogonal_(fnet_modules[-1].weight, gain=init.calculate_gain('relu'))  #返回给定的非线性函数的推荐增益值
        #if bnidx==k: fnet_modules.append(nn.BatchNorm1d(widths[k+1]))
        #fnet_modules.append(nn.ReLU(True))

    #fnet_modules.append(nn.Linear(widths[-2], widths[-1], bias=llbias))
    #if orthoinit: init.orthogonal_(fnet_modules[-1].weight)       #使得参数初始化为正交矩阵
    #if bnidx==len(widths)-1: fnet_modules.append(nn.BatchNorm1d(fnet_modules[-1].weight.size(0)))
    #return nn.Sequential(*fnet_modules)


class GraphNetwork(nn.Module):
    """ It is constructed in a flexible way based on `config` string,
    which contains sequence of comma-delimited layer definiton tokens layer_arg1_arg2_... See README.md for examples.
    """
    """它是基于config字符串以灵活的方式构造的，该字符串包含逗号分隔的图层定义标记序列layer_arg1_arg2_...。
    """
    def __init__(self, config, nfeat, fnet_widths, fnet_orthoinit=True, fnet_llbias=True, fnet_bnidx=-1, edge_mem_limit=1e20):
        super(GraphNetwork, self).__init__()
        self.gconvs = []

#……………………………………重写
        #definition tokens Layer_arg1_arg2_...

#1.0
        fnet = create_fnet(fnet_widths + [nfeat ** 2], fnet_orthoinit, fnet_llbias, fnet_bnidx)
        cell = GRUCellEx(nfeat, nfeat, bias=True, layernorm=True, ingate=True)
        gconv = RNNGraphConvModule(cell, fnet, nrepeats=10, cat_all=True, edge_mem_limit=edge_mem_limit)
        self.add_module(str(1), nn.Linear(nfeat, 8))
#2.0
        for d,conf in enumerate(config.split(',')):
            conf=conf.strip().split('_')
            if conf[0]=='f':
                self.add_module(str(1), nn.Linear(nfeat, 8))
            elif conf[0]=='gru':  #nfeat=32
                fnet = create_fnet(fnet_widths + [nfeat ** 2], fnet_orthoinit, fnet_llbias, fnet_bnidx)
                cell = GRUCellEx(nfeat, nfeat, bias=True, layernorm=True, ingate=True)
                gconv = RNNGraphConvModule(cell, fnet, nrepeats=10, cat_all=True, edge_mem_limit=edge_mem_limit)
                
#………………………………………………………………………………………………………………………………………………重写

        #config=['gru_10,f_8']
        #for(d=0),conf[0]='gru',conf[1]='10'
        #for(d-1),conf[1]='f',conf[1]='8'
        """
        for d, conf in enumerate(config.split(',')):
            conf = conf.strip().split('_')

            if conf[0]=='f':    #Fully connected layer;  args: output_feats
                self.add_module(str(d), nn.Linear(nfeat, int(conf[1])))
                nfeat = int(conf[1])
            elif conf[0]=='b':  #Batch norm;             args: not_affine
                self.add_module(str(d), nn.BatchNorm1d(nfeat, eps=1e-5, affine=len(conf)==1))
            elif conf[0]=='r':  #ReLU;
                self.add_module(str(d), nn.ReLU(True))
            elif conf[0]=='d':  #Dropout;                args: dropout_prob
                self.add_module(str(d), nn.Dropout(p=float(conf[1]), inplace=False))

            elif conf[0]=='crf': #ECC-CRF;               args: repeats
                nrepeats = int(conf[1])

                fnet = create_fnet(fnet_widths + [nfeat*nfeat], fnet_orthoinit, fnet_llbias, fnet_bnidx)
                gconv = ecc.GraphConvModule(nfeat, nfeat, fnet, edge_mem_limit=edge_mem_limit)
                crf = ECC_CRFModule(gconv, nrepeats)
                self.add_module(str(d), crf)
                self.gconvs.append(gconv)

            elif conf[0]=='gru' or conf[0]=='lstm': #RNN-ECC     args: repeats, vv=False, layernorm=True, ingate=True, cat_all=True
                nrepeats = int(conf[1])
                vv = bool(int(conf[2])) if len(conf)>2 else True # whether ECC does matrix-value mult or element-wise mult
                layernorm = bool(int(conf[3])) if len(conf)>3 else True
                ingate = bool(int(conf[4])) if len(conf)>4 else True
                cat_all = bool(int(conf[5])) if len(conf)>5 else True

                fnet = create_fnet(fnet_widths + [nfeat**2 if not vv else nfeat], fnet_orthoinit, fnet_llbias, fnet_bnidx)
                if conf[0]=='gru':
                    cell = GRUCellEx(nfeat, nfeat, bias=True, layernorm=layernorm, ingate=ingate)
                else:
                    cell = LSTMCellEx(nfeat, nfeat, bias=True, layernorm=layernorm, ingate=ingate)
                gconv = RNNGraphConvModule(cell, fnet, nrepeats=nrepeats, cat_all=cat_all, edge_mem_limit=edge_mem_limit)
                self.add_module(str(d), gconv)
                self.gconvs.append(gconv)
                if cat_all: nfeat *= nrepeats + 1

            elif len(conf[0])>0:
                raise NotImplementedError('Unknown module: ' + conf[0])
            """


    def set_info(self, gc_infos, cuda):
        """ Provides convolution modules with graph structure information for the current batch.
        """
        gc_infos = gc_infos if isinstance(gc_infos,(list,tuple)) else [gc_infos]
        for i,gc in enumerate(self.gconvs):
            if cuda: gc_infos[i].cuda()
            gc.set_info(gc_infos[i])

    def forward(self, input):
        for module in self._modules.values():
            input = module(input)
        return input

